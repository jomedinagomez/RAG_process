{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, ContentFormat\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, DocumentAnalysisFeature\n",
    "from azure.ai.documentintelligence.models import DocumentTable\n",
    "from azure.ai.documentintelligence.models import DocumentTable\n",
    "\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from prepdocslib.textsplitter import SentenceTextSplitter, SimpleTextSplitter\n",
    "from prepdocslib.page import Page\n",
    "from prepdocslib.parser import Parser\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mdpd\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import tiktoken\n",
    "import uuid\n",
    "import html\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Document Intelligence Client\n",
    "\n",
    "AZURE_DOC_INTELLIGENCE_ENDPOINT = os.environ[\"AZURE_DOC_INTELLIGENCE_ENDPOINT\"]\n",
    "AZURE_DOC_INTELLIGENCE_KEY = os.environ[\"AZURE_DOC_INTELLIGENCE_KEY\"]\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(endpoint=AZURE_DOC_INTELLIGENCE_ENDPOINT, credential=AzureKeyCredential(AZURE_DOC_INTELLIGENCE_KEY), api_version=\"2024-02-29-preview\")\n",
    "\n",
    "# Azure OpenAI Client\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-07-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_html(table):\n",
    "    table_html = \"<table>\"\n",
    "    rows = [\n",
    "        sorted([cell for cell in table.cells if cell.row_index == i], key=lambda cell: cell.column_index)\n",
    "        for i in range(table.row_count)\n",
    "    ]\n",
    "    for row_cells in rows:\n",
    "        table_html += \"<tr>\"\n",
    "        for cell in row_cells:\n",
    "            tag = \"th\" if (cell.kind == \"columnHeader\" or cell.kind == \"rowHeader\") else \"td\"\n",
    "            cell_spans = \"\"\n",
    "            if cell.column_span is not None and cell.column_span > 1:\n",
    "                cell_spans += f\" colSpan={cell.column_span}\"\n",
    "            if cell.row_span is not None and cell.row_span > 1:\n",
    "                cell_spans += f\" rowSpan={cell.row_span}\"\n",
    "            table_html += f\"<{tag}{cell_spans}>{html.escape(cell.content)}</{tag}>\"\n",
    "        table_html += \"</tr>\"\n",
    "    table_html += \"</table>\"\n",
    "    return table_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_html_processing(OcrExtractionDIOutput):\n",
    "    offset = 0\n",
    "    page_map = []\n",
    "    page_map_dict =[]\n",
    "\n",
    "    for page_num, page in enumerate(OcrExtractionDIOutput.pages):\n",
    "        tables_on_page = [\n",
    "            table\n",
    "            for table in (OcrExtractionDIOutput.tables or [])\n",
    "            if table.bounding_regions and table.bounding_regions[0].page_number == page_num + 1\n",
    "        ]\n",
    "        #print(tables_on_page)\n",
    "\n",
    "        # mark all positions of the table spans in the page\n",
    "        page_offset = page.spans[0].offset\n",
    "        page_length = page.spans[0].length\n",
    "        table_chars = [-1] * page_length\n",
    "        for table_id, table in enumerate(tables_on_page):\n",
    "            for span in table.spans:\n",
    "                # replace all table spans with \"table_id\" in table_chars array\n",
    "                for i in range(span.length):\n",
    "                    idx = span.offset - page_offset + i\n",
    "                    if idx >= 0 and idx < page_length:\n",
    "                        table_chars[idx] = table_id\n",
    "\n",
    "        # build page text by replacing characters in table spans with table html\n",
    "        page_text = \"\"\n",
    "        added_tables = set()\n",
    "        for idx, table_id in enumerate(table_chars):\n",
    "            if table_id == -1:\n",
    "                page_text += OcrExtractionDIOutput.content[page_offset + idx]\n",
    "            elif table_id not in added_tables:\n",
    "                page_text += table_to_html(tables_on_page[table_id])\n",
    "                added_tables.add(table_id)\n",
    "\n",
    "        page_text += \" \"\n",
    "        page_map.append((page_num+1, offset, page_text))\n",
    "\n",
    "        single_page_dict = {}\n",
    "        single_page_dict['page_num']= page_num+1\n",
    "        single_page_dict['content'] = page_text\n",
    "        single_page_dict['offset'] = offset\n",
    "        page_map_dict.append(single_page_dict)\n",
    "\n",
    "        offset += len(page_text)\n",
    "\n",
    "    return page_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombinePages(pagemap):\n",
    "    CombineText = \"\\n\".join(p['content'] for p in pagemap)\n",
    "    return CombineText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OcrExtractionDI(relative_path: str, Markdown: [bool]=True):\n",
    "    \n",
    "    path_to_document = os.path.abspath(\n",
    "        os.path.join(relative_path))\n",
    "    \n",
    "    if Markdown==True:\n",
    "        output_format = ContentFormat.MARKDOWN\n",
    "    else:\n",
    "        output_format = None\n",
    "\n",
    "    with open(path_to_document, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\"prebuilt-layout\", \n",
    "                                                                    analyze_request=f, content_type=\"application/octet-stream\", \n",
    "                                                                    output_content_format=output_format)\n",
    "    OcrExtractionDIOutput = poller.result()\n",
    "    \n",
    "    if Markdown==False:\n",
    "        pagemap = text_html_processing(OcrExtractionDIOutput)\n",
    "        extracted_processed_text = pagemap\n",
    "    else:\n",
    "        extracted_processed_text = OcrExtractionDIOutput\n",
    "\n",
    "    return extracted_processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_page(offset):\n",
    "    num_pages = len(ocr_extraction)\n",
    "    for i in range(num_pages - 1):\n",
    "        if offset >= ocr_extraction[i]['offset'] and offset < ocr_extraction[i + 1]['offset']:\n",
    "            return i\n",
    "    return num_pages - 1\n",
    "\n",
    "def FindEndFirstTable(text):\n",
    "    pattern = r\"</table>\"\n",
    "    matches_end = [match.start() for match in re.finditer(pattern, text)]\n",
    "    #print(matches_end)\n",
    "    #print(\"Found end of first table \" + str(matches_end[0]+len(pattern)))\n",
    "    return matches_end[0]+len(pattern)\n",
    "\n",
    "def FindEndLastClosedTable(text):\n",
    "    pattern = r\"</table>\"\n",
    "    matches_end = [match.start() for match in re.finditer(pattern, text)]\n",
    "    #print(matches_end)\n",
    "    #print(\"Found end of last closed table: \" + str(matches_end[-1]+len(pattern)))\n",
    "    return matches_end[-1]+len(pattern)\n",
    "\n",
    "def FindFirstOpenTableOffset(text):\n",
    "    pattern = r\"</table>\"\n",
    "    matches_end = [match.start() for match in re.finditer(pattern, text)]\n",
    "    matches_start = [match.start() for match in re.finditer(r\"<table\", text)]\n",
    "    if matches_end[0] < matches_start[0]:\n",
    "        #print(\"Warning: Section starts with an open table\")\n",
    "        additional_start_offset = matches_end[0]\n",
    "    else:\n",
    "        additional_start_offset = 0\n",
    "    return additional_start_offset\n",
    "\n",
    "def CustomTextSplitter(pagemap):\n",
    "    all_text = \"\\n\".join(p['content'] for p in pagemap)\n",
    "\n",
    "    DEFAULT_OVERLAP_PERCENT = 0  # See semantic search article for 10% overlap performance\n",
    "    DEFAULT_SECTION_LENGTH = 1500  # Roughly 400-500 tokens for English\n",
    "\n",
    "    STANDARD_WORD_BREAKS = [\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \"\\n\"]\n",
    "    # See W3C document https://www.w3.org/TR/jlreq/#cl-01\n",
    "    CJK_WORD_BREAKS = [\"、\",\"，\", \"；\", \"：\",\"（\", \"）\",\"【\",\"】\",\"「\",\"」\",\"『\",\"』\",\"〔\",\"〕\",\"〈\",\"〉\",\"《\",\"》\",\"〖\",\"〗\",\"〘\",\"〙\",\"〚\",\"〛\",\"〝\",\"〞\",\"〟\",\"〰\",\"–\",\"—\",\"‘\",\"’\",\"‚\",\"‛\",\"“\",\"”\",\"„\",\"‟\",\"‹\",\"›\",]\n",
    "    STANDARD_SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "    # See CL05 and CL06, based on JIS X 4051:2004\n",
    "    # https://www.w3.org/TR/jlreq/#cl-04\n",
    "    CJK_SENTENCE_ENDINGS = [\"。\", \"！\", \"？\", \"‼\", \"⁇\", \"⁈\", \"⁉\"]\n",
    "\n",
    "    sentence_endings = STANDARD_SENTENCE_ENDINGS + CJK_SENTENCE_ENDINGS\n",
    "    word_breaks = STANDARD_WORD_BREAKS + CJK_WORD_BREAKS\n",
    "    max_section_length = DEFAULT_SECTION_LENGTH\n",
    "    sentence_search_limit = 200\n",
    "    #max_tokens_per_section = max_tokens_per_section\n",
    "    section_overlap = 0\n",
    "\n",
    "    length = len(all_text)\n",
    "    start = 0\n",
    "    end = length\n",
    "\n",
    "    sections  = []\n",
    "    index = 0\n",
    "    raw_sections = []\n",
    "\n",
    "    while start + section_overlap < length:\n",
    "        last_word = -1\n",
    "        end = start + max_section_length\n",
    "\n",
    "        if end > length:\n",
    "            end = length\n",
    "        else:\n",
    "            # Try to find the end of the sentence\n",
    "            while (\n",
    "                end < length\n",
    "                and (end - start - max_section_length) < sentence_search_limit\n",
    "                and all_text[end] not in sentence_endings\n",
    "            ):\n",
    "                if all_text[end] in word_breaks:\n",
    "                    last_word = end\n",
    "                end += 1\n",
    "            if end < length and all_text[end] not in sentence_endings and last_word > 0:\n",
    "                end = last_word  # Fall back to at least keeping a whole word\n",
    "        if end < length:\n",
    "            end += 1\n",
    "\n",
    "        # Try to find the start of the sentence or at least a whole word boundary\n",
    "        '''\n",
    "        last_word = -1\n",
    "        while (\n",
    "            start > 0\n",
    "            and start > end - max_section_length - 2 * sentence_search_limit\n",
    "            and all_text[start] not in sentence_endings\n",
    "        ):\n",
    "            if all_text[start] in word_breaks:\n",
    "                last_word = start\n",
    "            start -= 1\n",
    "        if all_text[start] not in sentence_endings and last_word > 0:\n",
    "            start = last_word\n",
    "        if start > 0:\n",
    "            start += 1\n",
    "        '''\n",
    "\n",
    "        section_text = all_text[start:end]\n",
    "        \n",
    "        opened_tables = re.findall(r\"<table\", section_text, re.DOTALL)\n",
    "        closed_tables = re.findall(r\"</table>\", section_text, re.DOTALL)\n",
    "        raw_sections.append(section_text)\n",
    "        \n",
    "        if len(opened_tables) == 0:\n",
    "            #print(\"------------------------------------------------------------------------\")\n",
    "            #print(\"Index: \", index)\n",
    "            #print(\"---> No table found in chunk:\")\n",
    "            start = end\n",
    "            #print(\"New start is: \", start)\n",
    "            sections.append({\"content\": section_text,\"chunk_id\":index+1})\n",
    "        elif len(opened_tables) == 1:\n",
    "            #print(\"------------------------------------------------------------------------\")\n",
    "            #print(\"Index: \", index)\n",
    "            #print(\"---> {} Table found in chunk:\".format(len(opened_tables)))\n",
    "            if len(closed_tables) == len(opened_tables):\n",
    "                #print(\"---> {} Table found in chunk:\".format(len(closed_tables)))\n",
    "                section_text = all_text[start:end]\n",
    "                start = end\n",
    "                sections.append({\"content\": section_text,\"chunk_id\":index+1})\n",
    "                #print(\"New start is: \", start)\n",
    "            else:    \n",
    "                #print(\"Index: \", index)\n",
    "                #print(\"--------> 1 table not closed\")\n",
    "                #print(\"--------> Adding 5000 characters to the end of the table\")\n",
    "                incrased_section_text = all_text[start:end+5000]\n",
    "                end_first_table = FindEndFirstTable(incrased_section_text)\n",
    "                end = start + end_first_table\n",
    "                section_text = all_text[start:end]\n",
    "                start = end\n",
    "                #print(\"New start is: \", start)\n",
    "                sections.append({\"content\": section_text,\"chunk_id\":index+1})\n",
    "        else:\n",
    "            #print(\"------------------------------------------------------------------------\")\n",
    "            #print(\"Index: \", index)\n",
    "            #print(\"---> {} Table found in chunk:\".format(len(opened_tables)))\n",
    "            \n",
    "            if len(closed_tables) == len(opened_tables):\n",
    "                #print(\"---> {} Table found in chunk:\".format(len(closed_tables)))\n",
    "                section_text = all_text[start:end]\n",
    "                start = end\n",
    "                sections.append({\"content\": section_text,\"chunk_id\":index+1})\n",
    "                #print(\"New start is: \", start)\n",
    "            else:\n",
    "                #print(\"Index: \", index)\n",
    "                #print(\"--------> {} closed tables\".format(len(closed_tables)))\n",
    "                #print(\"--------> {} tables not closed\".format(len(opened_tables)-len(closed_tables)))\n",
    "                end_last_closed_table = FindEndLastClosedTable(section_text)\n",
    "                end = start + end_last_closed_table\n",
    "                section_text = all_text[start:end]\n",
    "                start = end\n",
    "                sections.append({\"content\": section_text,\"chunk_id\":index+1})\n",
    "                #print(\"New start is: \", start)\n",
    "        index = index + 1\n",
    "\n",
    "    if start + section_overlap < end:\n",
    "        sections.append({\"content\": all_text[start:end],\"chunk_id\":index+1})\n",
    "    return pd.DataFrame(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MdFormatting(ocr_extraction):\n",
    "    doc_string = ocr_extraction.content\n",
    "    strings_to_replace = re.findall(\".+\\n===\", doc_string)\n",
    "    for string in strings_to_replace:\n",
    "        doc_string = doc_string.replace(string, \"=== \"+string.replace(\"===\",\"\"))\n",
    "\n",
    "    ## Split the document into chunks base on markdown headers.\n",
    "    headers_to_split_on = [\n",
    "        (\"===\", \"Title\"),\n",
    "        (\"##\", \"Header 1\"),\n",
    "    ]\n",
    "    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    markdown_chunks = text_splitter.split_text(doc_string)\n",
    "\n",
    "    chunk_list = []\n",
    "    for chunk in markdown_chunks:\n",
    "        try:\n",
    "            title = chunk.metadata['Title']\n",
    "        except:\n",
    "            title = \"\"\n",
    "        try:\n",
    "            header1 = chunk.metadata['Header 1']\n",
    "        except:\n",
    "            header1 = \"\"\n",
    "\n",
    "        chunk_list.append({\"title\": title,\"header\":header1,\"content\": title + \"/n\" + header1 + \"/n\"+ chunk.page_content})\n",
    "    return pd.DataFrame(chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumberTokens(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateVector(text: str) -> list:\n",
    "    response = aoai_client.embeddings.create(\n",
    "        input = text,\n",
    "        model= \"text-embedding-3-small\")\n",
    "    return list(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSummary(content):\n",
    "    query = \"Can you extract a summary of following portion of a 10K filing? content: \" + content\n",
    "    messages = [{\"role\":\"system\",\"content\":\"You are an investment adivsor that reads information from SEC filings, such as 10K and 10Q. please be concise, please only provide a brief description with no explanation or detail\"}, \n",
    "               {\"role\":\"user\",\"content\":query}]\n",
    "\n",
    "    response = aoai_client.chat.completions.create(model=\"gpt-4o-mini\",  \n",
    "                                        messages = messages, \n",
    "                                        temperature=0,  \n",
    "                                        max_tokens=2000,\n",
    "                                        seed = 42)\n",
    "    summary = response.choices[0].message.content\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTitle(content):\n",
    "    query = \"Can you extract the title of following portion of a 10K filing? content: \" + content\n",
    "    messages = [{\"role\":\"system\",\"content\":\"You are an investment adivsor that reads information from SEC filings, such as 10K and 10Q. please be concise and do not generate any extra language, the reader will know that is reading a title\"}, \n",
    "               {\"role\":\"user\",\"content\":query}]\n",
    "\n",
    "    response = aoai_client.chat.completions.create(model=\"gpt-4o-mini\",  \n",
    "                                        messages = messages, \n",
    "                                        temperature=0,  \n",
    "                                        max_tokens=2000,\n",
    "                                        seed = 42)\n",
    "    title = response.choices[0].message.content\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateUniqueID():\n",
    "    return str(uuid.uuid4().fields[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveDFJson(DataFrame):\n",
    "    for i in DataFrame.index:\n",
    "        DataFrame.loc[i].to_json(\"../data/processed/json_file_{}.json\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReportPeriod(content):\n",
    "    query = \"Can you extract the period corresponding 10K/10Q filing? content: \" + content\n",
    "    messages = [{\"role\":\"system\",\"content\":\"You are an investment adivsor that reads information from SEC filings, such as 10K and 10Q. please be concise and do not generate any extra language. please generate the period in the format of 'YYYY-MM-DD'\"}, \n",
    "               {\"role\":\"user\",\"content\":query}]\n",
    "\n",
    "    response = aoai_client.chat.completions.create(model=\"gpt-4o-mini\",  \n",
    "                                        messages = messages, \n",
    "                                        temperature=0,  \n",
    "                                        max_tokens=2000,\n",
    "                                        seed = 42)\n",
    "    period = response.choices[0].message.content\n",
    "\n",
    "    return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessMD(ocr_extraction):\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 2: Extracting Markdown Output into DataFrame\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe =MdFormatting(ocr_extraction)\n",
    "    extracted_dataframe['char_len'] = extracted_dataframe.content.apply(lambda x: len(x))\n",
    "    extracted_dataframe['token_len'] = extracted_dataframe.content.apply(lambda x: NumberTokens(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 3: Vectorizing title\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['title_vector'] = extracted_dataframe.title.apply(lambda x: GenerateVector(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 4: Vectorizing content\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['content_vector'] = extracted_dataframe.content.apply(lambda x: GenerateVector(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 5: Generating Unique ID and Dropping Unnecesary Columns\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['chunk_id'] = [i+1 for i in range(len(extracted_dataframe))]\n",
    "    extracted_dataframe = extracted_dataframe.drop(columns = ['char_len','token_len','header'], errors = 'ignore')\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    return extracted_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessChunk(ocr_extraction):\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 2: Extracting Markdown Output into DataFrame\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe =CustomTextSplitter(ocr_extraction)\n",
    "    extracted_dataframe['char_len'] = extracted_dataframe.content.apply(lambda x: len(x))\n",
    "    extracted_dataframe['token_len'] = extracted_dataframe.content.apply(lambda x: NumberTokens(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 3: Create Title of each Section\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['title'] = extracted_dataframe.content.apply(lambda x: GetTitle(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 4: Vectorizing title\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['title_vector'] = extracted_dataframe.title.apply(lambda x: GenerateVector(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 5: Vectorizing content\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['content_vector'] = extracted_dataframe.content.apply(lambda x: GenerateVector(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 6: Generating Unique ID and Dropping Unnecesary Columns\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe = extracted_dataframe.drop(columns = ['char_len','token_len'], errors = 'ignore')\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    return extracted_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessPage(ocr_extraction):\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 2: Extracting Markdown Output into DataFrame\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe =pd.DataFrame(ocr_extraction).drop(columns=['offset'])\n",
    "    extracted_dataframe['char_len'] = extracted_dataframe.content.apply(lambda x: len(x))\n",
    "    extracted_dataframe['token_len'] = extracted_dataframe.content.apply(lambda x: NumberTokens(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 3: Create Title of each Section\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['title'] = extracted_dataframe.content.apply(lambda x: GetTitle(x))\n",
    "    extracted_dataframe['title'] = extracted_dataframe['title'].fillna(\"none\")\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 4: Vectorizing title\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['title_vector'] = extracted_dataframe.title.apply(lambda x: GenerateVector(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 5: Vectorizing content\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['content_vector'] = extracted_dataframe.content.apply(lambda x: GenerateVector(x))\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 6: Generating Unique ID and Dropping Unnecesary Columns\")\n",
    "    start = time.time()\n",
    "    extracted_dataframe['chunk_id'] = extracted_dataframe['page_num']\n",
    "    extracted_dataframe = extracted_dataframe.drop(columns = ['char_len','token_len'], errors = 'ignore')\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    return extracted_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on:  10K-AMZN-02-03-2023.pdf\n",
      "------------------------------------------------------------------------\n",
      "Step 1: OCR Extraction using Document Intelligence\n",
      "--> Total time:  20.321908712387085\n",
      "------------------------------------------------------------------------\n",
      "Step 2: Extracting Markdown Output into DataFrame\n",
      "--> Total time:  0.04969310760498047\n",
      "------------------------------------------------------------------------\n",
      "Step 3: Create Title of each Section\n",
      "--> Total time:  20.79076075553894\n",
      "------------------------------------------------------------------------\n",
      "Step 4: Vectorizing title\n",
      "--> Total time:  2.83966064453125\n",
      "------------------------------------------------------------------------\n",
      "Step 5: Vectorizing content\n",
      "--> Total time:  5.416094541549683\n",
      "------------------------------------------------------------------------\n",
      "Step 6: Generating Unique ID and Dropping Unnecesary Columns\n",
      "--> Total time:  0.0005152225494384766\n",
      "Working on:  10K-AMZN-02-04-2022.pdf\n",
      "------------------------------------------------------------------------\n",
      "Step 1: OCR Extraction using Document Intelligence\n",
      "--> Total time:  30.655375003814697\n",
      "------------------------------------------------------------------------\n",
      "Step 2: Extracting Markdown Output into DataFrame\n",
      "--> Total time:  0.10955452919006348\n",
      "------------------------------------------------------------------------\n",
      "Step 3: Create Title of each Section\n",
      "--> Total time:  51.83647918701172\n",
      "------------------------------------------------------------------------\n",
      "Step 4: Vectorizing title\n",
      "--> Total time:  7.115828514099121\n",
      "------------------------------------------------------------------------\n",
      "Step 5: Vectorizing content\n",
      "--> Total time:  14.25300121307373\n",
      "------------------------------------------------------------------------\n",
      "Step 6: Generating Unique ID and Dropping Unnecesary Columns\n",
      "--> Total time:  0.0\n",
      "Working on:  10K-GOOG-01-31-2024.pdf\n",
      "------------------------------------------------------------------------\n",
      "Step 1: OCR Extraction using Document Intelligence\n",
      "--> Total time:  24.996424198150635\n",
      "------------------------------------------------------------------------\n",
      "Step 2: Extracting Markdown Output into DataFrame\n",
      "--> Total time:  0.06470727920532227\n",
      "------------------------------------------------------------------------\n",
      "Step 3: Create Title of each Section\n",
      "--> Total time:  28.955087184906006\n",
      "------------------------------------------------------------------------\n",
      "Step 4: Vectorizing title\n",
      "--> Total time:  4.175674915313721\n",
      "------------------------------------------------------------------------\n",
      "Step 5: Vectorizing content\n"
     ]
    }
   ],
   "source": [
    "relative_path = \"../../data/raw/\"\n",
    "files = os.listdir(relative_path)\n",
    "\n",
    "Markdown = False\n",
    "PageIndex = True\n",
    "\n",
    "for Filename in files:\n",
    "\n",
    "    cleaned_filename = re.sub(r\".pdf\",'', Filename)\n",
    "    cleaned_filename.split(\"-\")\n",
    "    form_type = cleaned_filename.split(\"-\")[0]\n",
    "    ticker = cleaned_filename.split(\"-\")[1]\n",
    "    filing_date = cleaned_filename.split(\"-\")[4]+\"-\"+cleaned_filename.split(\"-\")[3]+\"-\"+cleaned_filename.split(\"-\")[2]\n",
    "    print(\"Working on: \", Filename)\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Step 1: OCR Extraction using Document Intelligence\")\n",
    "    start = time.time()\n",
    "    ocr_extraction = OcrExtractionDI(relative_path = \"../../data/raw/\"+Filename, Markdown=Markdown)\n",
    "    end = time.time()\n",
    "    print(\"--> Total time: \", end-start)\n",
    "\n",
    "    if Markdown==True:\n",
    "        outout_dataframe = ProcessMD(ocr_extraction)\n",
    "        outout_dataframe['preprocessing_pipeline'] = \"DI_MD_MarkDownTextSplitter\"\n",
    "    elif Markdown==False and PageIndex==False:\n",
    "        outout_dataframe = ProcessChunk(ocr_extraction)\n",
    "        outout_dataframe['preprocessing_pipeline'] = \"DI_Text_HTML_CustomTextSplitter\"\n",
    "    elif Markdown==False and PageIndex==True:\n",
    "        outout_dataframe = ProcessPage(ocr_extraction)\n",
    "        outout_dataframe['preprocessing_pipeline'] = \"DI_Text_HTML_PageSplitter\"\n",
    "\n",
    "    outout_dataframe['filename'] = re.sub(\".pdf\", \"\", Filename)\n",
    "    outout_dataframe['chunk_id'] = outout_dataframe.filename.astype(str)+\"-chunk-id-\"+outout_dataframe['chunk_id'].astype(str)\n",
    "    outout_dataframe['filing_period'] = GetReportPeriod(outout_dataframe['content'][0])\n",
    "    outout_dataframe['filing_date'] = filing_date\n",
    "    outout_dataframe['form_type'] = form_type\n",
    "    outout_dataframe['ticker'] = ticker\n",
    "\n",
    "    outout_dataframe.to_parquet(\"../../data/processed/files/\"+Filename+\".parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secdemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
