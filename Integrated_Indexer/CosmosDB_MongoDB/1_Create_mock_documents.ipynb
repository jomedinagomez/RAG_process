{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, ContentFormat\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, DocumentAnalysisFeature\n",
    "from azure.ai.documentintelligence.models import DocumentTable\n",
    "\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mdpd\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import tiktoken\n",
    "import uuid\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Document Intelligence Client\n",
    "\n",
    "AZURE_DOC_INTELLIGENCE_ENDPOINT = os.environ[\"AZURE_DOC_INTELLIGENCE_ENDPOINT\"]\n",
    "AZURE_DOC_INTELLIGENCE_KEY = os.environ[\"AZURE_DOC_INTELLIGENCE_KEY\"]\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(endpoint=AZURE_DOC_INTELLIGENCE_ENDPOINT, credential=AzureKeyCredential(AZURE_DOC_INTELLIGENCE_KEY), api_version=\"2024-02-29-preview\")\n",
    "\n",
    "# Azure OpenAI Client\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-07-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OcrExtractionDI(relative_path: str, Markdown: [bool]=True):\n",
    "    \n",
    "    path_to_document = os.path.abspath(\n",
    "        os.path.join(relative_path))\n",
    "    \n",
    "    if Markdown==True:\n",
    "        output_format = ContentFormat.MARKDOWN\n",
    "    else:\n",
    "        output_format = None\n",
    "\n",
    "    with open(path_to_document, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\"prebuilt-layout\", \n",
    "                                                                    analyze_request=f, content_type=\"application/octet-stream\", \n",
    "                                                                    output_content_format=output_format)\n",
    "    return poller.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MdFormatting(ocr_extraction):\n",
    "    doc_string = ocr_extraction.content\n",
    "    strings_to_replace = re.findall(\".+\\n===\", doc_string)\n",
    "    for string in strings_to_replace:\n",
    "        doc_string = doc_string.replace(string, \"=== \"+string.replace(\"===\",\"\"))\n",
    "\n",
    "    ## Split the document into chunks base on markdown headers.\n",
    "    headers_to_split_on = [\n",
    "        (\"===\", \"Title\"),\n",
    "        (\"##\", \"Header 1\"),\n",
    "    ]\n",
    "    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    markdown_chunks = text_splitter.split_text(doc_string)\n",
    "\n",
    "\n",
    "    chunk_list = []\n",
    "    for chunk in markdown_chunks:\n",
    "        try:\n",
    "            title = chunk.metadata['Title']\n",
    "        except:\n",
    "            title = \"\"\n",
    "        try:\n",
    "            header1 = chunk.metadata['Header 1']\n",
    "        except:\n",
    "            header1 = \"\"\n",
    "\n",
    "        chunk_list.append({\"title\": title,\"header\":header1,\"content\": title + \"/n\" + header1 + \"/n\"+ chunk.page_content})\n",
    "\n",
    "    return pd.DataFrame(chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumberTokens(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateVector(text: str) -> list:\n",
    "    response = aoai_client.embeddings.create(\n",
    "        input = text,\n",
    "        model= \"text-embedding-3-small\")\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSummary(content):\n",
    "    query = \"Can you extract a summary of following portion of a 10K filing? content: \" + content\n",
    "    messages = [{\"role\":\"system\",\"content\":\"You are an investment adivsor that reads information from SEC filings, such as 10K and 10Q. please be concise, please only provide a brief description with no explanation or detail\"}, \n",
    "               {\"role\":\"user\",\"content\":query}]\n",
    "\n",
    "    response = aoai_client.chat.completions.create(model=\"gpt-4o-mini\",  \n",
    "                                        messages = messages, \n",
    "                                        temperature=0,  \n",
    "                                        max_tokens=2000,\n",
    "                                        seed = 42)\n",
    "    summary = response.choices[0].message.content\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateUniqueID():\n",
    "    return str(uuid.uuid4().fields[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveDFJson(DataFrame):\n",
    "    for i in DataFrame.index:\n",
    "        DataFrame.loc[i].to_json(\"../data/processed/json_file_{}.json\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Step 1: OCR Extraction using Document Intelligence\n",
      "--> Total time:  26.315131187438965\n",
      "------------------------------------------------------------------------\n",
      "Step 2: Extracting Markdown Output into DataFrame\n",
      "--> Total time:  0.2090284824371338\n",
      "------------------------------------------------------------------------\n",
      "Step 3: Create Summary of each Section\n",
      "--> Total time:  155.95663690567017\n",
      "------------------------------------------------------------------------\n",
      "Step 4: Vectorizing title\n",
      "--> Total time:  3.852348804473877\n",
      "------------------------------------------------------------------------\n",
      "Step 5: Vectorizing content\n",
      "--> Total time:  7.335164785385132\n",
      "------------------------------------------------------------------------\n",
      "Step 6: Generating Unique ID and Dropping Unnecesary Columns\n",
      "--> Total time:  0.0004830360412597656\n",
      "------------------------------------------------------------------------\n",
      "Step 7: Storing documents as json\n",
      "--> Created a total of: [ 110 ] files\n",
      "--> Total time:  0.27649688720703125\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Step 1: OCR Extraction using Document Intelligence\")\n",
    "start = time.time()\n",
    "ocr_extraction = OcrExtractionDI(relative_path = \"../data/raw/MICROSOFT-10Q-FY2023-Q3.pdf\")\n",
    "end = time.time()\n",
    "print(\"--> Total time: \", end-start)\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Step 2: Extracting Markdown Output into DataFrame\")\n",
    "start = time.time()\n",
    "extracted_dataframe =MdFormatting(ocr_extraction)\n",
    "extracted_dataframe['char_len'] = extracted_dataframe.content.apply(lambda x: len(x))\n",
    "extracted_dataframe['token_len'] = extracted_dataframe.content.apply(lambda x: NumberTokens(x))\n",
    "end = time.time()\n",
    "print(\"--> Total time: \", end-start)\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Step 3: Create Summary of each Section\")\n",
    "start = time.time()\n",
    "extracted_dataframe['summary'] = extracted_dataframe.content.apply(lambda x: GetSummary(x))\n",
    "end = time.time()\n",
    "print(\"--> Total time: \", end-start)\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Step 4: Vectorizing title\")\n",
    "start = time.time()\n",
    "extracted_dataframe['title_vector'] = extracted_dataframe.title.apply(lambda x: GenerateVector(x))\n",
    "end = time.time()\n",
    "print(\"--> Total time: \", end-start)\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Step 5: Vectorizing content\")\n",
    "start = time.time()\n",
    "extracted_dataframe['content_vector'] = extracted_dataframe.content.apply(lambda x: GenerateVector(x))\n",
    "end = time.time()\n",
    "print(\"--> Total time: \", end-start)\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Step 6: Generating Unique ID and Dropping Unnecesary Columns\")\n",
    "start = time.time()\n",
    "extracted_dataframe['unique_id'] = extracted_dataframe.content.apply(lambda x: GenerateUniqueID())\n",
    "extracted_dataframe = extracted_dataframe.drop(columns = ['char_len','token_len'], errors = 'ignore')\n",
    "end = time.time()\n",
    "print(\"--> Total time: \", end-start)\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Step 7: Storing documents as json\")\n",
    "start = time.time()\n",
    "SaveDFJson(extracted_dataframe)\n",
    "end = time.time()\n",
    "print(\"--> Created a total of: [\", len(extracted_dataframe), \"] files\")\n",
    "print(\"--> Total time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secdemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
